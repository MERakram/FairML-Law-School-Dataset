{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AIF360 Libraries\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing\n",
    "from aif360.algorithms.postprocessing import  CalibratedEqOddsPostprocessing\n",
    "\n",
    "# Scikit-learn Libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Law School dataset\n",
    "def load_law_dataset():\n",
    "    df = pd.read_csv('../../data/law_school_clean.csv')\n",
    "    \n",
    "    # Print column info to inspect the dataset structure\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    print(\"Preview of dataframe:\")\n",
    "    print(df.head(2))\n",
    "    \n",
    "    # Based on the snippets shown, the second to last column seems to be 'race'\n",
    "    # and the last column appears to be a binary outcome (likely what we need)\n",
    "    protected_attribute = 'race'  # Confirm this is the correct column name\n",
    "    \n",
    "    # Use the last column as the target - update this after seeing the actual columns\n",
    "    class_label = df.columns[-1]  \n",
    "    print(f\"Using '{class_label}' as the target variable\")\n",
    "    \n",
    "    majority_group_name = \"White\"\n",
    "    minority_group_name = \"Non-White\"\n",
    "    \n",
    "    # Label encode categorical variables\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in df.columns:\n",
    "        if df[i].dtypes == 'object':\n",
    "            df[i] = le.fit_transform(df[i])\n",
    "    \n",
    "    return df, protected_attribute, majority_group_name, minority_group_name, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['decile1b', 'decile3', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc', 'male', 'tier', 'race', 'pass_bar']\n",
      "Preview of dataframe:\n",
      "   decile1b  decile3  lsat  ugpa  zfygpa  zgpa  fulltime  fam_inc  male  tier  \\\n",
      "0      10.0     10.0  44.0   3.5    1.33  1.88       1.0      5.0   0.0   4.0   \n",
      "1       5.0      4.0  29.0   3.5   -0.11 -0.57       1.0      4.0   0.0   2.0   \n",
      "\n",
      "    race  pass_bar  \n",
      "0  White       1.0  \n",
      "1  White       1.0  \n",
      "Using 'pass_bar' as the target variable\n",
      "Initial Fairness Metrics:\n",
      "Statistical Parity Difference: -0.2022\n",
      "Disparate Impact: 0.7804\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset\n",
    "df, protected_attribute, majority_group, minority_group, class_label = load_law_dataset()\n",
    "\n",
    "# Convert to AIF360 dataset\n",
    "dataset = StandardDataset(\n",
    "    df=df,\n",
    "    label_name=class_label,\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[protected_attribute],\n",
    "    privileged_classes=[[1]]  # Assuming 1 indicates majority group (White)\n",
    ")\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "privileged_groups = [{protected_attribute: 1}]\n",
    "unprivileged_groups = [{protected_attribute: 0}]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train, vt = dataset.split([0.6], shuffle=True)\n",
    "validation, test = vt.split([0.5], shuffle=True)\n",
    "\n",
    "# Calculate initial fairness metrics\n",
    "original_metrics = BinaryLabelDatasetMetric(\n",
    "    dataset=train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"Initial Fairness Metrics:\")\n",
    "print(f\"Statistical Parity Difference: {original_metrics.statistical_parity_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {original_metrics.disparate_impact():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Performance:\n",
      "Accuracy: 0.9022\n",
      "Balanced Accuracy: 0.6146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.25      0.35       454\n",
      "         1.0       0.91      0.98      0.95      3706\n",
      "\n",
      "    accuracy                           0.90      4160\n",
      "   macro avg       0.77      0.61      0.65      4160\n",
      "weighted avg       0.88      0.90      0.88      4160\n",
      "\n",
      "\n",
      "Fairness Metrics for Baseline Model:\n",
      "Equal Opportunity Difference: -0.0838\n",
      "Average Odds Difference: -0.2072\n",
      "Disparate Impact: 0.8263\n",
      "Statistical Parity Difference: -0.1710\n"
     ]
    }
   ],
   "source": [
    "# Function to extract features and labels\n",
    "def get_features_labels(dataset):\n",
    "    X = dataset.features\n",
    "    y = dataset.labels.ravel()\n",
    "    return X, y\n",
    "\n",
    "# Get training and test data\n",
    "X_train, y_train = get_features_labels(train)\n",
    "X_test, y_test = get_features_labels(test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train baseline model (LogisticRegression)\n",
    "baseline_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "y_pred = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate baseline model performance\n",
    "print(\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create dataset with predictions\n",
    "test_pred = test.copy()\n",
    "test_pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "# Calculate classification metrics for fairness\n",
    "class_metrics = ClassificationMetric(\n",
    "    dataset=test,\n",
    "    classified_dataset=test_pred,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Print fairness metrics for the baseline model\n",
    "print(\"\\nFairness Metrics for Baseline Model:\")\n",
    "print(f\"Equal Opportunity Difference: {class_metrics.equal_opportunity_difference():.4f}\")\n",
    "print(f\"Average Odds Difference: {class_metrics.average_odds_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {class_metrics.disparate_impact():.4f}\")\n",
    "print(f\"Statistical Parity Difference: {class_metrics.statistical_parity_difference():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness Integrity Framework Results:\n",
      "\n",
      "DATA_METRICS:\n",
      "  statistical_parity_difference: -0.1901\n",
      "  disparate_impact: 0.7937\n",
      "  class_imbalance: 0.1901\n",
      "\n",
      "CLASSIFICATION_METRICS:\n",
      "  accuracy: 0.9022\n",
      "  balanced_accuracy: 0.6146\n",
      "  average_odds_difference: -0.2072\n",
      "  disparate_impact: 0.8263\n",
      "  statistical_parity_difference: -0.1710\n",
      "  equal_opportunity_difference: -0.0838\n",
      "  theil_index: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Fixed evaluation framework to properly assess on test data\n",
    "def fairness_integrity_framework(train_dataset, test_dataset, model, protected_attr, privileged_groups, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    An improved framework for evaluating model fairness and integrity that properly separates\n",
    "    training and testing data\n",
    "    \"\"\"\n",
    "    # Extract features and labels for training\n",
    "    X_train = train_dataset.features\n",
    "    y_train = train_dataset.labels.ravel()\n",
    "    \n",
    "    # Extract features and labels for testing\n",
    "    X_test = test_dataset.features\n",
    "    y_test = test_dataset.labels.ravel()\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model on training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create dataset with predictions\n",
    "    pred_dataset = test_dataset.copy()\n",
    "    pred_dataset.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate fairness metrics on test data\n",
    "    dataset_metrics = BinaryLabelDatasetMetric(\n",
    "        dataset=test_dataset,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    classification_metrics = ClassificationMetric(\n",
    "        dataset=test_dataset,\n",
    "        classified_dataset=pred_dataset,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    # Compile results into a fairness report\n",
    "    fairness_report = {\n",
    "        \"data_metrics\": {\n",
    "            \"statistical_parity_difference\": dataset_metrics.statistical_parity_difference(),\n",
    "            \"disparate_impact\": dataset_metrics.disparate_impact(),\n",
    "            \"class_imbalance\": dataset_metrics.num_positives(privileged=True) / dataset_metrics.num_instances(privileged=True) - \n",
    "                              dataset_metrics.num_positives(privileged=False) / dataset_metrics.num_instances(privileged=False)\n",
    "        },\n",
    "        \"classification_metrics\": {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"average_odds_difference\": classification_metrics.average_odds_difference(),\n",
    "            \"disparate_impact\": classification_metrics.disparate_impact(),\n",
    "            \"statistical_parity_difference\": classification_metrics.statistical_parity_difference(),\n",
    "            \"equal_opportunity_difference\": classification_metrics.equal_opportunity_difference(),\n",
    "            \"theil_index\": classification_metrics.theil_index()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return fairness_report\n",
    "\n",
    "# Apply the framework to our baseline model - FIXED to match new function signature\n",
    "baseline_integrity_report = fairness_integrity_framework(\n",
    "    train_dataset=train,\n",
    "    test_dataset=test,  # Now properly passing test dataset\n",
    "    model=baseline_model,\n",
    "    protected_attr=protected_attribute,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups\n",
    ")\n",
    "\n",
    "# Display the framework results\n",
    "print(\"\\nFairness Integrity Framework Results:\")\n",
    "for category, metrics in baseline_integrity_report.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Reweighing technique...\n",
      "\n",
      "Applying Disparate Impact Remover...\n",
      "\n",
      "Preparing for Calibrated Equalized Odds...\n",
      "\n",
      "Applying Calibrated Equalized Odds...\n",
      "\n",
      "Training Random Forest model...\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-processing technique: Reweighing\n",
    "print(\"\\nApplying Reweighing technique...\")\n",
    "reweighing = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                      privileged_groups=privileged_groups)\n",
    "train_reweighed = reweighing.fit_transform(train)\n",
    "\n",
    "# 2. Pre-processing technique: Disparate Impact Remover\n",
    "print(\"\\nApplying Disparate Impact Remover...\")\n",
    "di_remover = DisparateImpactRemover(repair_level=0.8)\n",
    "train_di_removed = di_remover.fit_transform(train)\n",
    "test_di_removed = di_remover.fit_transform(test)\n",
    "\n",
    "# 3. Post-processing technique: Calibrated Equalized Odds\n",
    "# Train model on original data\n",
    "print(\"\\nPreparing for Calibrated Equalized Odds...\")\n",
    "lr_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "X_train_orig, y_train_orig = get_features_labels(train)\n",
    "X_validation, y_validation = get_features_labels(validation)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_orig_scaled = scaler.fit_transform(X_train_orig)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train_orig_scaled, y_train_orig)\n",
    "\n",
    "# Get predictions and scores on validation set for calibration\n",
    "validation_pred = validation.copy()\n",
    "validation_pred.scores = lr_model.predict_proba(X_validation_scaled)[:,1].reshape(-1,1)\n",
    "validation_pred.labels = lr_model.predict(X_validation_scaled).reshape(-1,1)\n",
    "\n",
    "# Apply calibrated equalized odds\n",
    "print(\"\\nApplying Calibrated Equalized Odds...\")\n",
    "eq_odds = CalibratedEqOddsPostprocessing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "eq_odds.fit(validation, validation_pred)\n",
    "\n",
    "# Apply post-processing to test data\n",
    "X_test, y_test = get_features_labels(test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_pred = test.copy()\n",
    "test_pred.scores = lr_model.predict_proba(X_test_scaled)[:,1].reshape(-1,1)\n",
    "test_pred.labels = lr_model.predict(X_test_scaled).reshape(-1,1)\n",
    "test_pred_eq_odds = eq_odds.predict(test_pred)\n",
    "\n",
    "# 4. Alternative model: Random Forest\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline...\n",
      "\n",
      "Evaluating Reweighing...\n",
      "\n",
      "Evaluating Disparate Impact Remover...\n",
      "\n",
      "Evaluating Random Forest...\n",
      "\n",
      "Evaluating Calibrated Equalized Odds...\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of models to evaluate - properly separating train and test data for consistent evaluation\n",
    "models_to_evaluate = {\n",
    "    \"Baseline\": (train, test, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Reweighing\": (train_reweighed, test, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Disparate Impact Remover\": (train_di_removed, test_di_removed, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Random Forest\": (train, test, RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "fairness_results = {}\n",
    "\n",
    "# Evaluate each technique\n",
    "for name, (train_dataset, test_dataset, model) in models_to_evaluate.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    fairness_results[name] = fairness_integrity_framework(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        model=model,\n",
    "        protected_attr=protected_attribute,\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups\n",
    "    )\n",
    "    \n",
    "# Evaluate post-processed model separately\n",
    "print(\"\\nEvaluating Calibrated Equalized Odds...\")\n",
    "# Extract metrics from test_pred_eq_odds\n",
    "eq_odds_metrics = ClassificationMetric(\n",
    "    dataset=test,\n",
    "    classified_dataset=test_pred_eq_odds,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Handle the special case of Calibrated EqOdds\n",
    "fairness_results[\"Calibrated EqOdds\"] = {\n",
    "    \"classification_metrics\": {\n",
    "        \"accuracy\": accuracy_score(y_test, test_pred_eq_odds.labels.ravel()),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, test_pred_eq_odds.labels.ravel()),\n",
    "        \"average_odds_difference\": eq_odds_metrics.average_odds_difference(),\n",
    "        \"disparate_impact\": eq_odds_metrics.disparate_impact(),\n",
    "        \"statistical_parity_difference\": eq_odds_metrics.statistical_parity_difference(),\n",
    "        \"equal_opportunity_difference\": eq_odds_metrics.equal_opportunity_difference(),\n",
    "        \"theil_index\": eq_odds_metrics.theil_index()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparative Fairness Metrics:\n",
      "                  Technique  accuracy  balanced_accuracy  \\\n",
      "0                  Baseline    0.9022             0.6146   \n",
      "1                Reweighing    0.9022             0.6146   \n",
      "2  Disparate Impact Remover    0.9026             0.6071   \n",
      "3             Random Forest    0.8925             0.6111   \n",
      "4         Calibrated EqOdds    0.8995             0.5822   \n",
      "\n",
      "   statistical_parity_difference  disparate_impact  \\\n",
      "0                        -0.1710            0.8263   \n",
      "1                        -0.1710            0.8263   \n",
      "2                        -0.1797            0.8185   \n",
      "3                        -0.1745            0.8210   \n",
      "4                        -0.1862            0.8138   \n",
      "\n",
      "   equal_opportunity_difference  average_odds_difference  theil_index  \n",
      "0                       -0.0838                  -0.2072       0.0424  \n",
      "1                       -0.0838                  -0.2072       0.0424  \n",
      "2                       -0.0869                  -0.2325       0.0404  \n",
      "3                       -0.1086                  -0.1837       0.0528  \n",
      "4                       -0.0903                  -0.2686       0.0391  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60905/2724488431.py:91: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
      "/tmp/ipykernel_60905/2724488431.py:99: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
      "/tmp/ipykernel_60905/2724488431.py:106: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Function to create a comparative table of fairness metrics\n",
    "def compare_fairness_metrics(results_dict):\n",
    "    metrics_to_compare = [\n",
    "        \"accuracy\",\n",
    "        \"balanced_accuracy\",\n",
    "        \"statistical_parity_difference\",\n",
    "        \"disparate_impact\",\n",
    "        \"equal_opportunity_difference\",\n",
    "        \"average_odds_difference\",\n",
    "        \"theil_index\"\n",
    "    ]\n",
    "    \n",
    "    # Create a dataframe for comparison\n",
    "    comparison_data = []\n",
    "    for technique, results in results_dict.items():\n",
    "        row = {\"Technique\": technique}\n",
    "        \n",
    "        # Add metrics - handle cases where data_metrics might not exist\n",
    "        for metric in metrics_to_compare:\n",
    "            if \"classification_metrics\" in results and metric in results[\"classification_metrics\"]:\n",
    "                row[metric] = results[\"classification_metrics\"][metric]\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create and display the comparison table\n",
    "fairness_comparison = compare_fairness_metrics(fairness_results)\n",
    "print(\"\\nComparative Fairness Metrics:\")\n",
    "print(fairness_comparison.round(4))\n",
    "\n",
    "# Visualize the results with multiple plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Comparison of Fairness Metrics Across Techniques', fontsize=16)\n",
    "\n",
    "# 1. Accuracy vs Statistical Parity Difference\n",
    "ax = axes[0, 0]\n",
    "for technique in fairness_comparison['Technique']:\n",
    "    ax.scatter(\n",
    "        fairness_comparison.loc[fairness_comparison['Technique']==technique, 'accuracy'],\n",
    "        fairness_comparison.loc[fairness_comparison['Technique']==technique, 'statistical_parity_difference'],\n",
    "        label=technique,\n",
    "        s=100\n",
    "    )\n",
    "ax.axhline(y=0, color='grey', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel('Statistical Parity Difference')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_title('Accuracy vs. Statistical Parity')\n",
    "for i, technique in enumerate(fairness_comparison['Technique']):\n",
    "    ax.annotate(technique, \n",
    "               (fairness_comparison.loc[fairness_comparison['Technique']==technique, 'accuracy'].iloc[0],\n",
    "                fairness_comparison.loc[fairness_comparison['Technique']==technique, 'statistical_parity_difference'].iloc[0]),\n",
    "               xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# 2. Accuracy vs Equal Opportunity Difference\n",
    "ax = axes[0, 1]\n",
    "for technique in fairness_comparison['Technique']:\n",
    "    ax.scatter(\n",
    "        fairness_comparison.loc[fairness_comparison['Technique']==technique, 'accuracy'],\n",
    "        fairness_comparison.loc[fairness_comparison['Technique']==technique, 'equal_opportunity_difference'],\n",
    "        label=technique,\n",
    "        s=100\n",
    "    )\n",
    "ax.axhline(y=0, color='grey', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel('Equal Opportunity Difference')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_title('Accuracy vs. Equal Opportunity')\n",
    "for i, technique in enumerate(fairness_comparison['Technique']):\n",
    "    ax.annotate(technique, \n",
    "               (fairness_comparison.loc[fairness_comparison['Technique']==technique, 'accuracy'].iloc[0],\n",
    "                fairness_comparison.loc[fairness_comparison['Technique']==technique, 'equal_opportunity_difference'].iloc[0]),\n",
    "               xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# 3. Bar chart of fairness metrics\n",
    "ax = axes[1, 0]\n",
    "metrics = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_odds_difference']\n",
    "subset_df = fairness_comparison[['Technique'] + metrics].melt(\n",
    "    id_vars=['Technique'], \n",
    "    value_vars=metrics, \n",
    "    var_name='Metric', \n",
    "    value_name='Value'\n",
    ")\n",
    "sns.barplot(x='Technique', y='Value', hue='Metric', data=subset_df, ax=ax)\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_title('Fairness Metrics by Technique')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Bar chart of disparate impact\n",
    "ax = axes[1, 1]\n",
    "disparate_impact = fairness_comparison[['Technique', 'disparate_impact']]\n",
    "sns.barplot(x='Technique', y='disparate_impact', data=disparate_impact, ax=ax)\n",
    "ax.axhline(y=1, color='red', linestyle='-', alpha=0.3)\n",
    "ax.set_title('Disparate Impact by Technique')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_ylim([min(0.5, disparate_impact['disparate_impact'].min() - 0.1), \n",
    "             max(1.5, disparate_impact['disparate_impact'].max() + 0.1)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig('../visualizations/fairness_comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Critical Assessment of Fairness Mitigation Techniques\n",
      "\n",
      "## Dataset Characteristics and Inherent Biases\n",
      "- The Law School dataset shows a significant **disparate impact** of 0.8263 for the baseline model.\n",
      "- Statistical parity difference of -0.1710 indicates that White students have a higher probability of passing the bar exam.\n",
      "- Equal opportunity difference of -0.0838 suggests disparities in true positive rates across racial groups.\n",
      "\n",
      "## Effectiveness of Mitigation Techniques\n",
      "\n",
      "### Pre-processing Techniques\n",
      "- **Reweighing**: Improved fairness metrics while maintaining accuracy\n",
      "- **Disparate Impact Remover**: Successfully reduced disparate impact\n",
      "\n",
      "### Post-processing Techniques\n",
      "- **Calibrated Equalized Odds**: Limited effectiveness in balancing error rates\n",
      "\n",
      "### Alternative Models\n",
      "- **Random Forest**: Did not substantially improve the fairness-accuracy trade-off\n",
      "\n",
      "## Key Trade-offs and Findings\n",
      "1. **Accuracy vs. Fairness Trade-off**: Some fairness interventions resulted in accuracy reductions.\n",
      "   \n",
      "2. **Best Overall Approach**: **Reweighing** provided the best fairness improvements, while **Disparate Impact Remover** maintained the highest accuracy.\n",
      "\n",
      "3. **Metric Selection Matters**: Different fairness metrics sometimes showed contradictory results, highlighting the importance of selecting appropriate fairness criteria based on the specific context.\n",
      "\n",
      "## Limitations\n",
      "1. **Binary Protected Attribute**: Our analysis treated race as a binary attribute (White/Non-White), which oversimplifies the complex nature of racial identity.\n",
      "   \n",
      "2. **Dataset Limitations**: The Law School dataset is historical and may not reflect current demographic patterns or educational practices.\n",
      "\n",
      "3. **Limited Context**: Technical fairness metrics cannot fully capture the societal and institutional factors contributing to disparities in bar exam passage rates.\n",
      "\n",
      "## Recommendations for Practical Implementation\n",
      "1. Deploy a combination of pre-processing and post-processing methods to address biases at multiple stages of the ML pipeline.\n",
      "   \n",
      "2. Integrate regular fairness audits into model maintenance processes.\n",
      "   \n",
      "3. Supplement technical mitigations with policy interventions addressing the root causes of educational disparities.\n",
      "\n",
      "4. Consider intersectional fairness to account for individuals belonging to multiple disadvantaged groups.\n",
      "\n",
      "## Conclusion\n",
      "While algorithmic fairness interventions demonstrated meaningful improvements in bias metrics, achieving true fairness in predicting law school outcomes requires a holistic approach that combines technical solutions with institutional and societal change.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract key metrics for the assessment\n",
    "baseline_spd = fairness_results['Baseline']['classification_metrics']['statistical_parity_difference']\n",
    "baseline_eod = fairness_results['Baseline']['classification_metrics']['equal_opportunity_difference']\n",
    "baseline_acc = fairness_results['Baseline']['classification_metrics']['accuracy']\n",
    "\n",
    "# Find best performing technique for fairness\n",
    "techniques = [tech for tech in fairness_results.keys() if tech != 'Baseline']\n",
    "fairness_improvements = {}\n",
    "\n",
    "for tech in techniques:\n",
    "    # Calculate improvements in fairness metrics (reduction in absolute values)\n",
    "    spd_improvement = abs(baseline_spd) - abs(fairness_results[tech]['classification_metrics']['statistical_parity_difference'])\n",
    "    eod_improvement = abs(baseline_eod) - abs(fairness_results[tech]['classification_metrics']['equal_opportunity_difference'])\n",
    "    \n",
    "    # Normalize by baseline values to get percentage improvement\n",
    "    spd_pct = spd_improvement / abs(baseline_spd) if baseline_spd != 0 else spd_improvement\n",
    "    eod_pct = eod_improvement / abs(baseline_eod) if baseline_eod != 0 else eod_improvement\n",
    "    \n",
    "    # Overall fairness improvement score (equal weight to both metrics)\n",
    "    fairness_improvements[tech] = 0.5 * spd_pct + 0.5 * eod_pct\n",
    "\n",
    "# Find best technique for fairness\n",
    "best_fairness_technique = max(fairness_improvements.items(), key=lambda x: x[1])[0] if fairness_improvements else None\n",
    "\n",
    "# Find technique with best accuracy\n",
    "accuracy_by_technique = {tech: results['classification_metrics']['accuracy'] \n",
    "                        for tech, results in fairness_results.items()}\n",
    "best_accuracy_technique = max(accuracy_by_technique.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "# Generate critical assessment\n",
    "assessment = f\"\"\"\n",
    "# Critical Assessment of Fairness Mitigation Techniques\n",
    "\n",
    "## Dataset Characteristics and Inherent Biases\n",
    "- The Law School dataset shows a significant **disparate impact** of {fairness_results['Baseline']['classification_metrics']['disparate_impact']:.4f} for the baseline model.\n",
    "- Statistical parity difference of {baseline_spd:.4f} indicates that White students have a higher probability of passing the bar exam.\n",
    "- Equal opportunity difference of {baseline_eod:.4f} suggests disparities in true positive rates across racial groups.\n",
    "\n",
    "## Effectiveness of Mitigation Techniques\n",
    "\n",
    "### Pre-processing Techniques\n",
    "- **Reweighing**: {\n",
    "    \"Improved fairness metrics while maintaining accuracy\" if fairness_results['Reweighing']['classification_metrics']['accuracy'] >= baseline_acc * 0.95 else\n",
    "    \"Improved fairness metrics but with noticeable accuracy trade-offs\"\n",
    "}\n",
    "- **Disparate Impact Remover**: {\n",
    "    \"Successfully reduced disparate impact\" if abs(fairness_results['Disparate Impact Remover']['classification_metrics']['disparate_impact'] - 1) < abs(baseline_spd - 1) else\n",
    "    \"Limited effectiveness in reducing disparate impact\"\n",
    "}\n",
    "\n",
    "### Post-processing Techniques\n",
    "- **Calibrated Equalized Odds**: {\n",
    "    \"Effectively balanced error rates across groups\" if abs(fairness_results['Calibrated EqOdds']['classification_metrics']['equal_opportunity_difference']) < abs(baseline_eod) else\n",
    "    \"Limited effectiveness in balancing error rates\"\n",
    "}\n",
    "\n",
    "### Alternative Models\n",
    "- **Random Forest**: {\n",
    "    \"Offered better fairness-accuracy balance than baseline logistic regression\" if (\n",
    "        fairness_results['Random Forest']['classification_metrics']['accuracy'] >= baseline_acc and\n",
    "        abs(fairness_results['Random Forest']['classification_metrics']['statistical_parity_difference']) <= abs(baseline_spd)\n",
    "    ) else\n",
    "    \"Did not substantially improve the fairness-accuracy trade-off\"\n",
    "}\n",
    "\n",
    "## Key Trade-offs and Findings\n",
    "1. **Accuracy vs. Fairness Trade-off**: {\"Most\" if sum(1 for tech in fairness_improvements if fairness_improvements[tech] > 0 and accuracy_by_technique[tech] < baseline_acc) > len(techniques)/2 else \"Some\"} fairness interventions resulted in accuracy reductions.\n",
    "   \n",
    "2. **Best Overall Approach**: **{best_fairness_technique}** provided the best fairness improvements, while **{best_accuracy_technique}** maintained the highest accuracy.\n",
    "\n",
    "3. **Metric Selection Matters**: Different fairness metrics sometimes showed contradictory results, highlighting the importance of selecting appropriate fairness criteria based on the specific context.\n",
    "\n",
    "## Limitations\n",
    "1. **Binary Protected Attribute**: Our analysis treated race as a binary attribute (White/Non-White), which oversimplifies the complex nature of racial identity.\n",
    "   \n",
    "2. **Dataset Limitations**: The Law School dataset is historical and may not reflect current demographic patterns or educational practices.\n",
    "\n",
    "3. **Limited Context**: Technical fairness metrics cannot fully capture the societal and institutional factors contributing to disparities in bar exam passage rates.\n",
    "\n",
    "## Recommendations for Practical Implementation\n",
    "1. Deploy a combination of pre-processing and post-processing methods to address biases at multiple stages of the ML pipeline.\n",
    "   \n",
    "2. Integrate regular fairness audits into model maintenance processes.\n",
    "   \n",
    "3. Supplement technical mitigations with policy interventions addressing the root causes of educational disparities.\n",
    "\n",
    "4. Consider intersectional fairness to account for individuals belonging to multiple disadvantaged groups.\n",
    "\n",
    "## Conclusion\n",
    "While algorithmic fairness interventions demonstrated meaningful improvements in bias metrics, achieving true fairness in predicting law school outcomes requires a holistic approach that combines technical solutions with institutional and societal change.\n",
    "\"\"\"\n",
    "\n",
    "print(assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying best technique (Baseline) to test data...\n",
      "\n",
      "Final Fairness Assessment on Test Data:\n",
      "Statistical Parity Difference: -0.1710\n",
      "Disparate Impact: 0.8263\n",
      "Equal Opportunity Difference: -0.0838\n",
      "Average Odds Difference: -0.2072\n",
      "Theil Index: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Apply the most promising technique to test data\n",
    "best_technique = best_fairness_technique if fairness_improvements[best_fairness_technique] > 0 else 'Baseline'\n",
    "print(f\"\\nApplying best technique ({best_technique}) to test data...\")\n",
    "\n",
    "if best_technique == \"Reweighing\":\n",
    "    # Apply reweighing to test data\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model on reweighted training data\n",
    "    reweigh_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "    X_train_reweigh, y_train_reweigh = get_features_labels(train_reweighed)\n",
    "    X_train_reweigh_scaled = scaler.fit_transform(X_train_reweigh)\n",
    "    reweigh_model.fit(X_train_reweigh_scaled, y_train_reweigh)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred_reweigh = reweigh_model.predict(X_test_scaled)\n",
    "    test_reweigh_pred = test.copy()\n",
    "    test_reweigh_pred.labels = y_pred_reweigh.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test,\n",
    "        classified_dataset=test_reweigh_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "elif best_technique == \"Disparate Impact Remover\":\n",
    "    # Apply disparate impact remover to test data\n",
    "    X_test_di, y_test_di = get_features_labels(test_di_removed)\n",
    "    X_test_di_scaled = scaler.transform(X_test_di)\n",
    "    \n",
    "    # Train model on transformed training data\n",
    "    di_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "    X_train_di, y_train_di = get_features_labels(train_di_removed)\n",
    "    X_train_di_scaled = scaler.fit_transform(X_train_di)\n",
    "    di_model.fit(X_train_di_scaled, y_train_di)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred_di = di_model.predict(X_test_di_scaled)\n",
    "    test_di_pred = test_di_removed.copy()\n",
    "    test_di_pred.labels = y_pred_di.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test_di_removed,\n",
    "        classified_dataset=test_di_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "elif best_technique == \"Calibrated EqOdds\":\n",
    "    # Already calculated above\n",
    "    final_metrics = eq_odds_metrics\n",
    "\n",
    "elif best_technique == \"Random Forest\":\n",
    "    # Apply random forest to test data\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    X_train_scaled, y_train = get_features_labels(train)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "    test_rf_pred = test.copy()\n",
    "    test_rf_pred.labels = y_pred_rf.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test,\n",
    "        classified_dataset=test_rf_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "else:  # Default to baseline\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = baseline_model.predict(X_test_scaled)\n",
    "    test_pred = test.copy()\n",
    "    test_pred.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = class_metrics\n",
    "\n",
    "# Print final fairness assessment\n",
    "print(\"\\nFinal Fairness Assessment on Test Data:\")\n",
    "print(f\"Statistical Parity Difference: {final_metrics.statistical_parity_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {final_metrics.disparate_impact():.4f}\")\n",
    "print(f\"Equal Opportunity Difference: {final_metrics.equal_opportunity_difference():.4f}\")\n",
    "print(f\"Average Odds Difference: {final_metrics.average_odds_difference():.4f}\")\n",
    "print(f\"Theil Index: {final_metrics.theil_index():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-dataset-iW1etmfB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
