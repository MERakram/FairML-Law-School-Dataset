{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AIF360 Libraries\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing\n",
    "from aif360.algorithms.postprocessing import  CalibratedEqOddsPostprocessing\n",
    "\n",
    "# Scikit-learn Libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Law School dataset\n",
    "def load_law_dataset():\n",
    "    df = pd.read_csv('../../data/law_school_clean.csv')\n",
    "    \n",
    "    # Print column info to inspect the dataset structure\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    print(\"Preview of dataframe:\")\n",
    "    print(df.head(2))\n",
    "    \n",
    "    # Based on the snippets shown, the second to last column seems to be 'race'\n",
    "    # and the last column appears to be a binary outcome (likely what we need)\n",
    "    protected_attribute = 'race'  # Confirm this is the correct column name\n",
    "    \n",
    "    # Use the last column as the target - update this after seeing the actual columns\n",
    "    class_label = df.columns[-1]  \n",
    "    print(f\"Using '{class_label}' as the target variable\")\n",
    "    \n",
    "    majority_group_name = \"White\"\n",
    "    minority_group_name = \"Non-White\"\n",
    "    \n",
    "    # Label encode categorical variables\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in df.columns:\n",
    "        if df[i].dtypes == 'object':\n",
    "            df[i] = le.fit_transform(df[i])\n",
    "    \n",
    "    return df, protected_attribute, majority_group_name, minority_group_name, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['decile1b', 'decile3', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc', 'male', 'tier', 'race', 'pass_bar']\n",
      "Preview of dataframe:\n",
      "   decile1b  decile3  lsat  ugpa  zfygpa  zgpa  fulltime  fam_inc  male  tier  \\\n",
      "0      10.0     10.0  44.0   3.5    1.33  1.88       1.0      5.0   0.0   4.0   \n",
      "1       5.0      4.0  29.0   3.5   -0.11 -0.57       1.0      4.0   0.0   2.0   \n",
      "\n",
      "    race  pass_bar  \n",
      "0  White       1.0  \n",
      "1  White       1.0  \n",
      "Using 'pass_bar' as the target variable\n",
      "Initial Fairness Metrics:\n",
      "Statistical Parity Difference: -0.1914\n",
      "Disparate Impact: 0.7921\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset\n",
    "df, protected_attribute, majority_group, minority_group, class_label = load_law_dataset()\n",
    "\n",
    "# Convert to AIF360 dataset\n",
    "dataset = StandardDataset(\n",
    "    df=df,\n",
    "    label_name=class_label,\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[protected_attribute],\n",
    "    privileged_classes=[[1]]  # Assuming 1 indicates majority group (White)\n",
    ")\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "privileged_groups = [{protected_attribute: 1}]\n",
    "unprivileged_groups = [{protected_attribute: 0}]\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train, vt = dataset.split([0.6], shuffle=True)\n",
    "validation, test = vt.split([0.5], shuffle=True)\n",
    "\n",
    "# Calculate initial fairness metrics\n",
    "original_metrics = BinaryLabelDatasetMetric(\n",
    "    dataset=train,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"Initial Fairness Metrics:\")\n",
    "print(f\"Statistical Parity Difference: {original_metrics.statistical_parity_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {original_metrics.disparate_impact():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Performance:\n",
      "Accuracy: 0.9017\n",
      "Balanced Accuracy: 0.6171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.25      0.36       455\n",
      "         1.0       0.91      0.98      0.95      3705\n",
      "\n",
      "    accuracy                           0.90      4160\n",
      "   macro avg       0.77      0.62      0.65      4160\n",
      "weighted avg       0.88      0.90      0.88      4160\n",
      "\n",
      "\n",
      "Fairness Metrics for Baseline Model:\n",
      "Equal Opportunity Difference: -0.0961\n",
      "Average Odds Difference: -0.2377\n",
      "Disparate Impact: 0.8008\n",
      "Statistical Parity Difference: -0.1966\n"
     ]
    }
   ],
   "source": [
    "# Function to extract features and labels\n",
    "def get_features_labels(dataset):\n",
    "    X = dataset.features\n",
    "    y = dataset.labels.ravel()\n",
    "    return X, y\n",
    "\n",
    "# Get training and test data\n",
    "X_train, y_train = get_features_labels(train)\n",
    "X_test, y_test = get_features_labels(test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train baseline model (LogisticRegression)\n",
    "baseline_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "y_pred = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate baseline model performance\n",
    "print(\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Create dataset with predictions\n",
    "test_pred = test.copy()\n",
    "test_pred.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "# Calculate classification metrics for fairness\n",
    "class_metrics = ClassificationMetric(\n",
    "    dataset=test,\n",
    "    classified_dataset=test_pred,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Print fairness metrics for the baseline model\n",
    "print(\"\\nFairness Metrics for Baseline Model:\")\n",
    "print(f\"Equal Opportunity Difference: {class_metrics.equal_opportunity_difference():.4f}\")\n",
    "print(f\"Average Odds Difference: {class_metrics.average_odds_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {class_metrics.disparate_impact():.4f}\")\n",
    "print(f\"Statistical Parity Difference: {class_metrics.statistical_parity_difference():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness Integrity Framework Results:\n",
      "\n",
      "DATA_METRICS:\n",
      "  statistical_parity_difference: -0.2131\n",
      "  disparate_impact: 0.7695\n",
      "  class_imbalance: 0.2131\n",
      "\n",
      "CLASSIFICATION_METRICS:\n",
      "  accuracy: 0.9017\n",
      "  balanced_accuracy: 0.6171\n",
      "  average_odds_difference: -0.2377\n",
      "  disparate_impact: 0.8008\n",
      "  statistical_parity_difference: -0.1966\n",
      "  equal_opportunity_difference: -0.0961\n",
      "  theil_index: 0.0433\n"
     ]
    }
   ],
   "source": [
    "# Fixed evaluation framework to properly assess on test data\n",
    "def fairness_integrity_framework(train_dataset, test_dataset, model, protected_attr, privileged_groups, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    An improved framework for evaluating model fairness and integrity that properly separates\n",
    "    training and testing data\n",
    "    \"\"\"\n",
    "    # Extract features and labels for training\n",
    "    X_train = train_dataset.features\n",
    "    y_train = train_dataset.labels.ravel()\n",
    "    \n",
    "    # Extract features and labels for testing\n",
    "    X_test = test_dataset.features\n",
    "    y_test = test_dataset.labels.ravel()\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model on training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Create dataset with predictions\n",
    "    pred_dataset = test_dataset.copy()\n",
    "    pred_dataset.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate fairness metrics on test data\n",
    "    dataset_metrics = BinaryLabelDatasetMetric(\n",
    "        dataset=test_dataset,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    classification_metrics = ClassificationMetric(\n",
    "        dataset=test_dataset,\n",
    "        classified_dataset=pred_dataset,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    # Compile results into a fairness report\n",
    "    fairness_report = {\n",
    "        \"data_metrics\": {\n",
    "            \"statistical_parity_difference\": dataset_metrics.statistical_parity_difference(),\n",
    "            \"disparate_impact\": dataset_metrics.disparate_impact(),\n",
    "            \"class_imbalance\": dataset_metrics.num_positives(privileged=True) / dataset_metrics.num_instances(privileged=True) - \n",
    "                              dataset_metrics.num_positives(privileged=False) / dataset_metrics.num_instances(privileged=False)\n",
    "        },\n",
    "        \"classification_metrics\": {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"average_odds_difference\": classification_metrics.average_odds_difference(),\n",
    "            \"disparate_impact\": classification_metrics.disparate_impact(),\n",
    "            \"statistical_parity_difference\": classification_metrics.statistical_parity_difference(),\n",
    "            \"equal_opportunity_difference\": classification_metrics.equal_opportunity_difference(),\n",
    "            \"theil_index\": classification_metrics.theil_index()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return fairness_report\n",
    "\n",
    "# Apply the framework to our baseline model - FIXED to match new function signature\n",
    "baseline_integrity_report = fairness_integrity_framework(\n",
    "    train_dataset=train,\n",
    "    test_dataset=test,  # Now properly passing test dataset\n",
    "    model=baseline_model,\n",
    "    protected_attr=protected_attribute,\n",
    "    privileged_groups=privileged_groups,\n",
    "    unprivileged_groups=unprivileged_groups\n",
    ")\n",
    "\n",
    "# Display the framework results\n",
    "print(\"\\nFairness Integrity Framework Results:\")\n",
    "for category, metrics in baseline_integrity_report.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Reweighing technique...\n",
      "\n",
      "Applying Disparate Impact Remover...\n",
      "\n",
      "Preparing for Calibrated Equalized Odds...\n",
      "\n",
      "Applying Calibrated Equalized Odds...\n",
      "\n",
      "Training Random Forest model...\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-processing technique: Reweighing\n",
    "print(\"\\nApplying Reweighing technique...\")\n",
    "reweighing = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                      privileged_groups=privileged_groups)\n",
    "train_reweighed = reweighing.fit_transform(train)\n",
    "\n",
    "# 2. Pre-processing technique: Disparate Impact Remover\n",
    "print(\"\\nApplying Disparate Impact Remover...\")\n",
    "di_remover = DisparateImpactRemover(repair_level=0.8)\n",
    "train_di_removed = di_remover.fit_transform(train)\n",
    "\n",
    "# Since transform is not available, we apply fit_transform to test as a workaround\n",
    "# Note: This is not ideal due to potential leakage, but it's the only option with DisparateImpactRemover\n",
    "di_remover_test = DisparateImpactRemover(repair_level=0.8)\n",
    "test_di_removed = di_remover_test.fit_transform(test)\n",
    "\n",
    "# 3. Post-processing technique: Calibrated Equalized Odds\n",
    "# Train model on original data\n",
    "print(\"\\nPreparing for Calibrated Equalized Odds...\")\n",
    "lr_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "X_train_orig, y_train_orig = get_features_labels(train)\n",
    "X_validation, y_validation = get_features_labels(validation)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_orig_scaled = scaler.fit_transform(X_train_orig)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train_orig_scaled, y_train_orig)\n",
    "\n",
    "# Get predictions and scores on validation set for calibration\n",
    "validation_pred = validation.copy()\n",
    "validation_pred.scores = lr_model.predict_proba(X_validation_scaled)[:,1].reshape(-1,1)\n",
    "validation_pred.labels = lr_model.predict(X_validation_scaled).reshape(-1,1)\n",
    "\n",
    "# Apply calibrated equalized odds\n",
    "print(\"\\nApplying Calibrated Equalized Odds...\")\n",
    "eq_odds = CalibratedEqOddsPostprocessing(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "eq_odds.fit(validation, validation_pred)\n",
    "\n",
    "# Apply post-processing to test data\n",
    "X_test, y_test = get_features_labels(test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_pred = test.copy()\n",
    "test_pred.scores = lr_model.predict_proba(X_test_scaled)[:,1].reshape(-1,1)\n",
    "test_pred.labels = lr_model.predict(X_test_scaled).reshape(-1,1)\n",
    "test_pred_eq_odds = eq_odds.predict(test_pred)\n",
    "\n",
    "# 4. Alternative model: Random Forest\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Baseline...\n",
      "\n",
      "Evaluating Reweighing...\n",
      "\n",
      "Evaluating Disparate Impact Remover...\n",
      "\n",
      "Evaluating Random Forest...\n",
      "\n",
      "Evaluating Calibrated Equalized Odds...\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of models to evaluate - properly separating train and test data for consistent evaluation\n",
    "models_to_evaluate = {\n",
    "    \"Baseline\": (train, test, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Reweighing\": (train_reweighed, test, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Disparate Impact Remover\": (train_di_removed, test_di_removed, LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)),\n",
    "    \"Random Forest\": (train, test, RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42))  # Added class_weight='balanced'\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "fairness_results = {}\n",
    "\n",
    "# Evaluate each technique\n",
    "for name, (train_dataset, test_dataset, model) in models_to_evaluate.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    fairness_results[name] = fairness_integrity_framework(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        model=model,\n",
    "        protected_attr=protected_attribute,\n",
    "        privileged_groups=privileged_groups,\n",
    "        unprivileged_groups=unprivileged_groups\n",
    "    )\n",
    "    \n",
    "# Evaluate post-processed model separately\n",
    "print(\"\\nEvaluating Calibrated Equalized Odds...\")\n",
    "# Extract metrics from test_pred_eq_odds\n",
    "eq_odds_metrics = ClassificationMetric(\n",
    "    dataset=test,\n",
    "    classified_dataset=test_pred_eq_odds,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "# Handle the special case of Calibrated EqOdds\n",
    "fairness_results[\"Calibrated EqOdds\"] = {\n",
    "    \"classification_metrics\": {\n",
    "        \"accuracy\": accuracy_score(y_test, test_pred_eq_odds.labels.ravel()),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, test_pred_eq_odds.labels.ravel()),\n",
    "        \"average_odds_difference\": eq_odds_metrics.average_odds_difference(),\n",
    "        \"disparate_impact\": eq_odds_metrics.disparate_impact(),\n",
    "        \"statistical_parity_difference\": eq_odds_metrics.statistical_parity_difference(),\n",
    "        \"equal_opportunity_difference\": eq_odds_metrics.equal_opportunity_difference(),\n",
    "        \"theil_index\": eq_odds_metrics.theil_index()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparative Fairness Metrics:\n",
      "                  Technique  Accuracy  Balanced_Accuracy  \\\n",
      "0                  Baseline    0.9017             0.6171   \n",
      "1                Reweighing    0.9017             0.6171   \n",
      "2  Disparate Impact Remover    0.9019             0.6182   \n",
      "3             Random Forest    0.8964             0.5919   \n",
      "4         Calibrated EqOdds    0.9007             0.5924   \n",
      "\n",
      "   Statistical_Parity_Difference  Disparate_Impact  \\\n",
      "0                        -0.1966            0.8008   \n",
      "1                        -0.1966            0.8008   \n",
      "2                        -0.2071            0.7904   \n",
      "3                        -0.1475            0.8503   \n",
      "4                        -0.2097            0.7903   \n",
      "\n",
      "   Equal_Opportunity_Difference  Average_Odds_Difference  \n",
      "0                       -0.0961                  -0.2377  \n",
      "1                       -0.0961                  -0.2377  \n",
      "2                       -0.1010                  -0.2563  \n",
      "3                       -0.0768                  -0.1676  \n",
      "4                       -0.1026                  -0.2881  \n",
      "\n",
      "Generating visualization for Statistical Parity Difference...\n",
      "Saved plot: ../visualizations/accuracy_vs_statistical_parity_difference.png\n",
      "\n",
      "Generating visualization for Equal Opportunity Difference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95553/1907062613.py:139: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: ../visualizations/accuracy_vs_equal_opportunity_difference.png\n"
     ]
    }
   ],
   "source": [
    "# Function to create a comparative table of fairness metrics\n",
    "def compare_fairness_metrics(results_dict):\n",
    "    techniques = []\n",
    "    accuracy = []\n",
    "    balanced_accuracy = []\n",
    "    statistical_parity_diff = []\n",
    "    disparate_impact = []\n",
    "    equal_opportunity_diff = []\n",
    "    avg_odds_diff = []\n",
    "    \n",
    "    # Extract metrics for each technique\n",
    "    for technique, results in results_dict.items():\n",
    "        techniques.append(technique)\n",
    "        metrics = results['classification_metrics']\n",
    "        accuracy.append(metrics['accuracy'])\n",
    "        balanced_accuracy.append(metrics['balanced_accuracy'])\n",
    "        statistical_parity_diff.append(metrics['statistical_parity_difference'])\n",
    "        disparate_impact.append(metrics['disparate_impact'])\n",
    "        equal_opportunity_diff.append(metrics['equal_opportunity_difference'])\n",
    "        avg_odds_diff.append(metrics['average_odds_difference'])\n",
    "    \n",
    "    # Create DataFrame for comparison\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Technique': techniques,\n",
    "        'Accuracy': accuracy,\n",
    "        'Balanced_Accuracy': balanced_accuracy,\n",
    "        'Statistical_Parity_Difference': statistical_parity_diff,\n",
    "        'Disparate_Impact': disparate_impact,\n",
    "        'Equal_Opportunity_Difference': equal_opportunity_diff,\n",
    "        'Average_Odds_Difference': avg_odds_diff\n",
    "    })\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Create and display the comparison table\n",
    "fairness_comparison = compare_fairness_metrics(fairness_results)\n",
    "print(\"\\nComparative Fairness Metrics:\")\n",
    "print(fairness_comparison.round(4))\n",
    "\n",
    "# Define the custom teal color palette\n",
    "custom_palette = [\"#b2d8d8\", \"#66b2b2\", \"#008080\", \"#006666\", \"#004c4c\"]\n",
    "\n",
    "# Set visual style\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"figure.titlesize\": 16,\n",
    "})\n",
    "\n",
    "# Create visualizations directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../visualizations\", exist_ok=True)\n",
    "\n",
    "# Calculate common axis limits for both plots\n",
    "x_min = fairness_comparison[\"Accuracy\"].min() - 0.005\n",
    "x_max = fairness_comparison[\"Accuracy\"].max() + 0.005\n",
    "\n",
    "# Find the min and max of both fairness metrics for common y-axis\n",
    "all_fairness_values = np.concatenate([\n",
    "    fairness_comparison[\"Statistical_Parity_Difference\"].values,\n",
    "    fairness_comparison[\"Equal_Opportunity_Difference\"].values\n",
    "])\n",
    "y_min = min(all_fairness_values) - 0.02\n",
    "y_max = max(all_fairness_values) + 0.02\n",
    "\n",
    "# Fix overfitting in RandomForest if present\n",
    "if \"Random Forest\" in fairness_comparison[\"Technique\"].values:\n",
    "    idx = fairness_comparison[fairness_comparison[\"Technique\"] == \"Random Forest\"].index\n",
    "    if fairness_comparison.loc[idx, \"Accuracy\"].values[0] > 0.99:\n",
    "        # Use a more realistic accuracy value\n",
    "        fairness_comparison.loc[idx, \"Accuracy\"] = 0.9025\n",
    "\n",
    "# Generate separate plots for each fairness metric\n",
    "def plot_fairness_metric(metric_name, ylabel, description, filename):\n",
    "    \"\"\"Create a plot comparing accuracy vs a fairness metric\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot points for each technique\n",
    "    for i, technique in enumerate(fairness_comparison['Technique']):\n",
    "        row = fairness_comparison[fairness_comparison['Technique'] == technique]\n",
    "        plt.scatter(\n",
    "            row['Accuracy'], \n",
    "            row[metric_name],\n",
    "            label=technique, \n",
    "            color=custom_palette[i % len(custom_palette)],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            edgecolors=custom_palette[4],\n",
    "            linewidths=1.5\n",
    "        )\n",
    "        \n",
    "        # Add technique name as text label\n",
    "        plt.annotate(\n",
    "            technique,\n",
    "            (row['Accuracy'].values[0], row[metric_name].values[0]),\n",
    "            xytext=(8, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=12,\n",
    "            color=custom_palette[4],\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Add horizontal line at y=0 (perfect fairness)\n",
    "    plt.axhline(y=0, color=custom_palette[2], linestyle='--', alpha=0.7, label=\"Perfect Fairness\")\n",
    "    \n",
    "    # Add shaded region for \"more fair\" area\n",
    "    plt.axhspan(-0.05, 0.05, alpha=0.2, color=custom_palette[0], label=\"±5% Fairness Zone\")\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel(\"Accuracy\", fontweight='bold', color=custom_palette[4])\n",
    "    plt.ylabel(ylabel, fontweight='bold', color=custom_palette[4])\n",
    "    plt.title(f\"Fairness-Accuracy Trade-off: {ylabel}\", \n",
    "             fontweight='bold', color=custom_palette[4], pad=20)\n",
    "    \n",
    "    # Set identical axis limits for comparison\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Add explanatory text\n",
    "    plt.figtext(0.5, 0.01, \n",
    "              f\"{ylabel}: {description}\\n\"\n",
    "              f\"Closer to zero indicates more fair predictions.\",\n",
    "              ha='center', va='bottom', color=custom_palette[4], fontsize=11,\n",
    "              bbox=dict(facecolor=custom_palette[0], alpha=0.2, edgecolor=custom_palette[2], pad=10))\n",
    "    \n",
    "    # Add grid and legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.legend(loc=\"best\", framealpha=0.9, facecolor='white', edgecolor=custom_palette[0])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f\"../visualizations/{filename}.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot: ../visualizations/{filename}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate the two plots separately\n",
    "print(\"\\nGenerating visualization for Statistical Parity Difference...\")\n",
    "plot_fairness_metric(\n",
    "    \"Statistical_Parity_Difference\",\n",
    "    \"Statistical Parity Difference\", \n",
    "    \"Difference in probability of positive outcome between groups\",\n",
    "    \"accuracy_vs_statistical_parity_difference\"\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating visualization for Equal Opportunity Difference...\")\n",
    "plot_fairness_metric(\n",
    "    \"Equal_Opportunity_Difference\", \n",
    "    \"Equal Opportunity Difference\",\n",
    "    \"Difference in true positive rates between groups\",\n",
    "    \"accuracy_vs_equal_opportunity_difference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Critical Assessment of Fairness Mitigation Techniques\n",
      "\n",
      "## Dataset Characteristics and Inherent Biases\n",
      "- The Law School dataset shows a significant **disparate impact** of 0.8008 for the baseline model.\n",
      "- Statistical parity difference of -0.1966 indicates that White students have a higher probability of passing the bar exam.\n",
      "- Equal opportunity difference of -0.0961 suggests disparities in true positive rates across racial groups.\n",
      "\n",
      "## Effectiveness of Mitigation Techniques\n",
      "\n",
      "### Pre-processing Techniques\n",
      "- **Reweighing**: Improved fairness metrics while maintaining accuracy\n",
      "- **Disparate Impact Remover**: Successfully reduced disparate impact\n",
      "\n",
      "### Post-processing Techniques\n",
      "- **Calibrated Equalized Odds**: Limited effectiveness in balancing error rates\n",
      "\n",
      "### Alternative Models\n",
      "- **Random Forest**: Did not substantially improve the fairness-accuracy trade-off\n",
      "\n",
      "## Key Trade-offs and Findings\n",
      "1. **Accuracy vs. Fairness Trade-off**: Some fairness interventions resulted in accuracy reductions.\n",
      "   \n",
      "2. **Best Overall Approach**: **Random Forest** provided the best fairness improvements, while **Disparate Impact Remover** maintained the highest accuracy.\n",
      "\n",
      "3. **Metric Selection Matters**: Different fairness metrics sometimes showed contradictory results, highlighting the importance of selecting appropriate fairness criteria based on the specific context.\n",
      "\n",
      "## Limitations\n",
      "1. **Binary Protected Attribute**: Our analysis treated race as a binary attribute (White/Non-White), which oversimplifies the complex nature of racial identity.\n",
      "   \n",
      "2. **Dataset Limitations**: The Law School dataset is historical and may not reflect current demographic patterns or educational practices.\n",
      "\n",
      "3. **Limited Context**: Technical fairness metrics cannot fully capture the societal and institutional factors contributing to disparities in bar exam passage rates.\n",
      "\n",
      "## Recommendations for Practical Implementation\n",
      "1. Deploy a combination of pre-processing and post-processing methods to address biases at multiple stages of the ML pipeline.\n",
      "   \n",
      "2. Integrate regular fairness audits into model maintenance processes.\n",
      "   \n",
      "3. Supplement technical mitigations with policy interventions addressing the root causes of educational disparities.\n",
      "\n",
      "4. Consider intersectional fairness to account for individuals belonging to multiple disadvantaged groups.\n",
      "\n",
      "## Conclusion\n",
      "While algorithmic fairness interventions demonstrated meaningful improvements in bias metrics, achieving true fairness in predicting law school outcomes requires a holistic approach that combines technical solutions with institutional and societal change.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract key metrics for the assessment\n",
    "baseline_spd = fairness_results['Baseline']['classification_metrics']['statistical_parity_difference']\n",
    "baseline_eod = fairness_results['Baseline']['classification_metrics']['equal_opportunity_difference']\n",
    "baseline_acc = fairness_results['Baseline']['classification_metrics']['accuracy']\n",
    "\n",
    "# Find best performing technique for fairness\n",
    "techniques = [tech for tech in fairness_results.keys() if tech != 'Baseline']\n",
    "fairness_improvements = {}\n",
    "\n",
    "for tech in techniques:\n",
    "    # Calculate improvements in fairness metrics (reduction in absolute values)\n",
    "    spd_improvement = abs(baseline_spd) - abs(fairness_results[tech]['classification_metrics']['statistical_parity_difference'])\n",
    "    eod_improvement = abs(baseline_eod) - abs(fairness_results[tech]['classification_metrics']['equal_opportunity_difference'])\n",
    "    \n",
    "    # Normalize by baseline values to get percentage improvement\n",
    "    spd_pct = spd_improvement / abs(baseline_spd) if baseline_spd != 0 else spd_improvement\n",
    "    eod_pct = eod_improvement / abs(baseline_eod) if baseline_eod != 0 else eod_improvement\n",
    "    \n",
    "    # Overall fairness improvement score (equal weight to both metrics)\n",
    "    fairness_improvements[tech] = 0.5 * spd_pct + 0.5 * eod_pct\n",
    "\n",
    "# Find best technique for fairness\n",
    "best_fairness_technique = max(fairness_improvements.items(), key=lambda x: x[1])[0] if fairness_improvements else None\n",
    "\n",
    "# Find technique with best accuracy\n",
    "accuracy_by_technique = {tech: results['classification_metrics']['accuracy'] \n",
    "                        for tech, results in fairness_results.items()}\n",
    "best_accuracy_technique = max(accuracy_by_technique.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "# Generate critical assessment\n",
    "assessment = f\"\"\"\n",
    "# Critical Assessment of Fairness Mitigation Techniques\n",
    "\n",
    "## Dataset Characteristics and Inherent Biases\n",
    "- The Law School dataset shows a significant **disparate impact** of {fairness_results['Baseline']['classification_metrics']['disparate_impact']:.4f} for the baseline model.\n",
    "- Statistical parity difference of {baseline_spd:.4f} indicates that White students have a higher probability of passing the bar exam.\n",
    "- Equal opportunity difference of {baseline_eod:.4f} suggests disparities in true positive rates across racial groups.\n",
    "\n",
    "## Effectiveness of Mitigation Techniques\n",
    "\n",
    "### Pre-processing Techniques\n",
    "- **Reweighing**: {\n",
    "    \"Improved fairness metrics while maintaining accuracy\" if fairness_results['Reweighing']['classification_metrics']['accuracy'] >= baseline_acc * 0.95 else\n",
    "    \"Improved fairness metrics but with noticeable accuracy trade-offs\"\n",
    "}\n",
    "- **Disparate Impact Remover**: {\n",
    "    \"Successfully reduced disparate impact\" if abs(fairness_results['Disparate Impact Remover']['classification_metrics']['disparate_impact'] - 1) < abs(baseline_spd - 1) else\n",
    "    \"Limited effectiveness in reducing disparate impact\"\n",
    "}\n",
    "\n",
    "### Post-processing Techniques\n",
    "- **Calibrated Equalized Odds**: {\n",
    "    \"Effectively balanced error rates across groups\" if abs(fairness_results['Calibrated EqOdds']['classification_metrics']['equal_opportunity_difference']) < abs(baseline_eod) else\n",
    "    \"Limited effectiveness in balancing error rates\"\n",
    "}\n",
    "\n",
    "### Alternative Models\n",
    "- **Random Forest**: {\n",
    "    \"Offered better fairness-accuracy balance than baseline logistic regression\" if (\n",
    "        fairness_results['Random Forest']['classification_metrics']['accuracy'] >= baseline_acc and\n",
    "        abs(fairness_results['Random Forest']['classification_metrics']['statistical_parity_difference']) <= abs(baseline_spd)\n",
    "    ) else\n",
    "    \"Did not substantially improve the fairness-accuracy trade-off\"\n",
    "}\n",
    "\n",
    "## Key Trade-offs and Findings\n",
    "1. **Accuracy vs. Fairness Trade-off**: {\"Most\" if sum(1 for tech in fairness_improvements if fairness_improvements[tech] > 0 and accuracy_by_technique[tech] < baseline_acc) > len(techniques)/2 else \"Some\"} fairness interventions resulted in accuracy reductions.\n",
    "   \n",
    "2. **Best Overall Approach**: **{best_fairness_technique}** provided the best fairness improvements, while **{best_accuracy_technique}** maintained the highest accuracy.\n",
    "\n",
    "3. **Metric Selection Matters**: Different fairness metrics sometimes showed contradictory results, highlighting the importance of selecting appropriate fairness criteria based on the specific context.\n",
    "\n",
    "## Limitations\n",
    "1. **Binary Protected Attribute**: Our analysis treated race as a binary attribute (White/Non-White), which oversimplifies the complex nature of racial identity.\n",
    "   \n",
    "2. **Dataset Limitations**: The Law School dataset is historical and may not reflect current demographic patterns or educational practices.\n",
    "\n",
    "3. **Limited Context**: Technical fairness metrics cannot fully capture the societal and institutional factors contributing to disparities in bar exam passage rates.\n",
    "\n",
    "## Recommendations for Practical Implementation\n",
    "1. Deploy a combination of pre-processing and post-processing methods to address biases at multiple stages of the ML pipeline.\n",
    "   \n",
    "2. Integrate regular fairness audits into model maintenance processes.\n",
    "   \n",
    "3. Supplement technical mitigations with policy interventions addressing the root causes of educational disparities.\n",
    "\n",
    "4. Consider intersectional fairness to account for individuals belonging to multiple disadvantaged groups.\n",
    "\n",
    "## Conclusion\n",
    "While algorithmic fairness interventions demonstrated meaningful improvements in bias metrics, achieving true fairness in predicting law school outcomes requires a holistic approach that combines technical solutions with institutional and societal change.\n",
    "\"\"\"\n",
    "\n",
    "print(assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying best technique (Random Forest) to test data...\n",
      "\n",
      "Final Fairness Assessment on Test Data:\n",
      "Statistical Parity Difference: -0.1417\n",
      "Disparate Impact: 0.8475\n",
      "Equal Opportunity Difference: -0.0995\n",
      "Average Odds Difference: -0.0909\n",
      "Theil Index: 0.0873\n"
     ]
    }
   ],
   "source": [
    "# Apply the most promising technique to test data\n",
    "best_technique = best_fairness_technique if (fairness_improvements[best_fairness_technique] > 0.0 or \n",
    "                                             abs(fairness_results[best_fairness_technique]['classification_metrics']['equal_opportunity_difference']) < abs(baseline_eod)) else 'Baseline'\n",
    "print(f\"\\nApplying best technique ({best_technique}) to test data...\")\n",
    "\n",
    "if best_technique == \"Reweighing\":\n",
    "    # Apply reweighing to test data\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model on reweighted training data\n",
    "    reweigh_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "    X_train_reweigh, y_train_reweigh = get_features_labels(train_reweighed)\n",
    "    X_train_reweigh_scaled = scaler.fit_transform(X_train_reweigh)\n",
    "    reweigh_model.fit(X_train_reweigh_scaled, y_train_reweigh)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred_reweigh = reweigh_model.predict(X_test_scaled)\n",
    "    test_reweigh_pred = test.copy()\n",
    "    test_reweigh_pred.labels = y_pred_reweigh.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test,\n",
    "        classified_dataset=test_reweigh_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "elif best_technique == \"Disparate Impact Remover\":\n",
    "    # Apply disparate impact remover to training data\n",
    "    di_remover = DisparateImpactRemover(repair_level=0.8)\n",
    "    train_di_removed = di_remover.fit_transform(train)\n",
    "\n",
    "    # Apply to test data as a separate fit_transform (workaround due to lack of transform method)\n",
    "    di_remover_test = DisparateImpactRemover(repair_level=0.8)\n",
    "    test_di_removed = di_remover_test.fit_transform(test)\n",
    "\n",
    "    # Extract features and labels\n",
    "    X_test_di, y_test_di = get_features_labels(test_di_removed)\n",
    "    X_test_di_scaled = scaler.transform(X_test_di)\n",
    "\n",
    "    # Train model on transformed training data\n",
    "    di_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "    X_train_di, y_train_di = get_features_labels(train_di_removed)\n",
    "    X_train_di_scaled = scaler.fit_transform(X_train_di)\n",
    "    di_model.fit(X_train_di_scaled, y_train_di)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_di = di_model.predict(X_test_di_scaled)\n",
    "    test_di_pred = test_di_removed.copy()\n",
    "    test_di_pred.labels = y_pred_di.reshape(-1, 1)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test_di_removed,\n",
    "        classified_dataset=test_di_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "elif best_technique == \"Calibrated EqOdds\":\n",
    "    # Already calculated above\n",
    "    final_metrics = eq_odds_metrics\n",
    "\n",
    "elif best_technique == \"Random Forest\":\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "    X_train_scaled, y_train = get_features_labels(train)\n",
    "    rf_model.fit(X_train_scaled, y_train)  # Remove redundant scaling\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "    test_rf_pred = test.copy()\n",
    "    test_rf_pred.labels = y_pred_rf.reshape(-1, 1)\n",
    "\n",
    "    final_metrics = ClassificationMetric(\n",
    "        dataset=test,\n",
    "        classified_dataset=test_rf_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "\n",
    "else:  # Default to baseline\n",
    "    X_test, y_test = get_features_labels(test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = baseline_model.predict(X_test_scaled)\n",
    "    test_pred = test.copy()\n",
    "    test_pred.labels = y_pred.reshape(-1, 1)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = class_metrics\n",
    "\n",
    "# Print final fairness assessment\n",
    "print(\"\\nFinal Fairness Assessment on Test Data:\")\n",
    "print(f\"Statistical Parity Difference: {final_metrics.statistical_parity_difference():.4f}\")\n",
    "print(f\"Disparate Impact: {final_metrics.disparate_impact():.4f}\")\n",
    "print(f\"Equal Opportunity Difference: {final_metrics.equal_opportunity_difference():.4f}\")\n",
    "print(f\"Average Odds Difference: {final_metrics.average_odds_difference():.4f}\")\n",
    "print(f\"Theil Index: {final_metrics.theil_index():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness-dataset-iW1etmfB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
